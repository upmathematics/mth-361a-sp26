---
title: "Discrete Random Variabes & <br>Probability Mass Functions"
subtitle: "Applied Statistics"
author: "MTH-361A | Spring 2026 | University of Portland"
output:
  slidy_presentation:
    font_adjustment: +2
    footer: "| MTH-361A Spring 2026 | <a href='../../index.html'>Back to the Course Website</a>"
    css: ../_style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Objectives

:::: {.column width=15%}
::::

:::: {.column width=70%}
- **Develop an understanding of the geometric random variable**
- **Develop an understanding of the binomial random variable**
- **Know how to compute probabilities of the binomial and geometric random variables**
::::

:::: {.column width=15%}
::::

## Independent and Identically Distributed

A sequence of random variables $X_1, X_2, \cdots, X_n$ is **independent and identically distributed (i.i.d)** if:

* *Identically distributed:* Each $X_i$ follows the same probability distribution
* *Independence:* The random variables do not influence each other

:::: {.column width=50%}
**Examples:**

* Tossing a fair coin multiple times
* Rolling a fair die multiple times
::::

:::: {.column width=49%}
**Why is i.i.d. Important?**

* Forms the foundation of probability and statistics (e.g., Law of Large Numbers, Central Limit Theorem).
::::

## Multiple Bernoulli Trials

A sequence of **multiple Bernoulli trials** consists i.i.d. Bernoulli r.v.s where each follows a Bernoulli distribution with "success" probability $p$.

* This leads to important distributions:
  - Geometric Distribution (number of trials until the first success)
  - Binomial Distribution (number of "success" outcomes in a fixed number of trials)

::: {style="color: red;"}
$\star$ Trials are independent (i.e., one outcome does not affect the next).
:::

## The Geometric R.V.

A **geometric r.v.** is a discrete r.v. that represents the number of Bernoulli trials until the first "success" where each trial is independent, with a fixed "success" probability $p$: $$X \sim \text{Geom}(p)$$

:::: {.column width=50%}
**Sample Space:**

\[
\begin{aligned}
1 & \longrightarrow 0 \text{ fail until success} \\
0,1 & \longrightarrow 1 \text{ fail until success} \\
0,0,1 & \longrightarrow 2 \text{ fail until success} \\
 & \vdots \\
0,0,0,\cdots,1 & \longrightarrow k \text{ fail until success} \\
\end{aligned}
\]
::::

:::: {.column width=49%}
**Probabilities:**

\[
\begin{aligned}
1 & \longrightarrow (1-p)^0 p \\
0,1 & \longrightarrow (1-p)^1 p \\
0,0,1 & \longrightarrow (1-p)^2 p \\
 & \vdots \\
0,0,0,\cdots,1 & \longrightarrow (1-p)^k p \\
\end{aligned}
\]
::::

::: {style="color: red;"}
$\star$ The geometric random variable counts the number of "failures" before a "success" and can also be viewed as counting the number of trials including the first "success."
:::

## The Geometric R.V.: PMF

The geometric r.v. $X \sim \text{Geom}(p)$ has infinite possible outcomes (or infinite sized sample space) where $p$ is the "success" probability.

The PMF of the geometric r.v. can be written in two ways: 

* *k "failures" until "success":* $$P(X=k) = (1-p)^k p, \ k = 0,1,2, \cdots$$
* *k trials including 1st "success":* $$P(X=k) = (1-p)^{k-1} p, \ k = 1,2, \cdots$$

::: {style="color: red;"}
$\star$ The geometric random variable models a situation where samples are taken with replacement, and the number of failures until the first success is counted.
:::

## The Geometric R.V.: Examples

* *What is the probability of "success" on the 6th trial with $p=\frac{1}{2}$? $$P(X=5) = \left(1-\frac{1}{2}\right)^5 \left(\frac{1}{2}\right) \approx 0.016$$ because there are 5 "failures" before the 6th trial.*

* *What is the probability that the first "success" occurs before the 6th trial, given $p=\frac{1}{2}$?*
$$
\begin{aligned}
P(X \le 5) & = \sum_{i=0}^5 P(X = i) \\ 
           & = \sum_{i=0}^5 \left(1-\frac{1}{2}\right)^{i} \left(\frac{1}{2}\right) \\ 
P(X \le 5) & \approx 0.984 \\
\end{aligned}
$$
because we need to count five or fewer "failures" before a "success" occurs.

## Why the Geometric R.V. Matters?

The Geometric R.V. models key scenarios used in:

:::: {.column width=15%}
::::
 
:::: {.column width=70%}
* *Medical trials*: Estimating the number of contacts a person has with an infected individual before they themselves become infected.
* *Quality control*: Finding the number of items inspected before encountering the first defective product.
* *Finance*: Analyzing the number of insurance claims made before a large loss occurs.
* *Machine learning*: Optimizing how many iterations are necessary before a model fits into a given data set.
* $\cdots$ etcetera
::::

:::: {.column width=15%}
::::

::: {style="color: red;"}
$\star$ It is the baseline of modeling the number of independent Bernoulli trials needed to achieve the first "success".
:::

## Disease Transmission

Suppose a scientist is estimating the number of contacts a person can have before they get infected with a contagious disease. Each contact is independent and each person encountered is unique.

Define the random variable:

$$
X = \text{the number of contacts before infection}
$$

Assume the probability of infection is $p = 0.01$.

Then, $$X \sim \text{Geom}(0.01).$$

::: {style="color: red;"}
$\star$ This is a Geometric r.v. because it is counting the number contacts without infection ("failures") before a contact with infection ("success") under multiple independent Bernoulli trials.
:::

## The Binomial R.V.

A **binomial r.v.** is a discrete r.v. representing the number of "success" in $n$ independent Bernoulli trials, each with "success" probability $p$: $$X \sim \text{Binom}(n,p)$$

:::: {.column width=50%}
**Sample Space:**

Suppose $n = 3$.

\[
\begin{aligned}
0,0,0 & \longrightarrow 3 \text{ fail and } 0 \text{ success} \\
0,0,1 & \longrightarrow 2 \text{ fail and } 1 \text{ success} \\
0,1,0 & \longrightarrow 2 \text{ fail and } 1 \text{ success} \\
0,1,1 & \longrightarrow 1 \text{ fail and } 2 \text{ success} \\
1,0,0 & \longrightarrow 2 \text{ fail and } 1 \text{ success} \\
1,0,1 & \longrightarrow 1 \text{ fail and } 2 \text{ success} \\
1,1,0 & \longrightarrow 1 \text{ fail and } 2 \text{ success} \\
1,1,1 & \longrightarrow 0 \text{ fail and } 3 \text{ success} \\
\end{aligned}
\]
::::

:::: {.column width=49%}
**Probabilities:**

Suppose $n = 3$.
\[
\begin{aligned}
0,0,0 & \longrightarrow (1-p)^3 p^0 \\
0,0,1 & \longrightarrow (1-p)^2 p^1 \\
0,1,0 & \longrightarrow (1-p)^2 p^1 \\
1,0,0 & \longrightarrow (1-p)^2 p^1 \\
0,1,1 & \longrightarrow (1-p)^1 p^2 \\
1,0,1 & \longrightarrow (1-p)^1 p^2 \\
1,1,0 & \longrightarrow (1-p)^1 p^2 \\
1,1,1 & \longrightarrow (1-p)^0 p^3
\end{aligned}
\]
::::

::: {style="color: red;"}
$\star$ The binomial random variable counts the number of "successes" in $n$ independent Bernoulli trials, where each trial has a "success" probability $p$.
:::

## Counting the Number of Combinations

:::: {.column width=49%}
**Permutations**

An arrangement of objects in a specific order. For $n$ objects, we pick $k$ objects to permute with **number of permutations** given by $$_n P_k = \frac{n!}{(n-k)!}.$$
::::

:::: {.column width=50%}
**Combinations**

A selection of objects where order does not matter. For $n$ objects, we pick $k$ objects to combine with **number of combinations** given by $$_n C_k = \binom{n}{k} = \frac{n!}{k!(n-k)!}.$$
::::

## The Binomial Coefficient

The **binomial coefficient**, denoted as $\binom{n}{k}$ represents the number of ways to choose $k$ objects from a set of $n$ objects without regard to order. It is given by the formula: $$\binom{n}{k} = \frac{n!}{k!(n-k)!}$$

Expanding binomial expressions using the **Binomial Theorem**: $$(x+y)^n = \sum_{k=0}^n \binom{n}{k} y^k x^{n-k}$$

If we let $x=1-p$ and $y=p$ (Bernoulli PMF), then $$(1-p+p)^n = \sum_{k=0}^n \binom{n}{k} p^k (1-p)^{n-k} = 1.$$

::: {style="color: red;"}
$\star$ Since $p$ is the "success" probability and the Binomial Theorem reduces to $1$, then this satisfies the probability axioms.
:::

## The Binomial R.V.: PMF

The binomial r.v. $X \sim \text{Binom}(n,p)$ has finite possible outcomes with PMF given by $$P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}, \ k = 0,1,2, \cdots, n$$ where $p$ is the "success" probability. The term $\binom{n}{k} = \frac{n!}{k! (n-k)!}$ is the binomial coefficient.

::: {style="color: red;"}
$\star$ The binomial random variable models a situation where samples are taken with replacement, and the number of successes is counted within a finite number of trials.
:::

## The Binomial R.V.: Examples

* *What is the probability of getting 4 "success" in 10 trials with $p=\frac{1}{2}$?* $$P(X=4) =\binom{10}{4} \left(\frac{1}{2}\right)^4 \left(1-\frac{1}{2}\right)^{n-k} \approx 0.205$$

* *What is the probability of getting at most 4 "success" in 10 trials with $p=\frac{1}{2}$?*
$$
\begin{aligned}
P(X \le 4) & = \sum_{i=0}^4 P(X = i) \\ 
           & = \sum_{i=0}^4 \binom{10}{i} \left(\frac{1}{2}\right)^i \left(1-\frac{1}{2}\right)^{10-i} \\
P(X \le 4) & \approx 0.377 \\
\end{aligned}
$$
because we need to count four or fewer "success" in 10 trials.

## Why the Binomial R.V. Matters?

The Binomial R.V. models key scenarios used in:

:::: {.column width=15%}
::::
 
:::: {.column width=70%}
* *Medical trials*: Testing a new vaccine on $200$ participants, where each participant either develops immunity (success) or does not (failure).
* *Quality control*: Randomly selecting $200$ microchips from a large batch and counting the number of defective microchips.
* *Finance*: Counting the number of "up" days of a stock over the next $200$ days.
* *Machine learning*: Identifying $200$ independent emails whether they are spam or not.
* $\cdots$ etcetera
::::

:::: {.column width=15%}
::::

::: {style="color: red;"}
$\star$ It is the baseline for modeling number of "success" outcomes in a fixed number of independent trials.
:::

## Disease Prevalence

Suppose a scientist is estimating the number of infected individuals within a population. They take $200$ individuals as a random sample and count how many have the infection. Each individual is independently sampled and unique.

Define the random variable:

$$
X = \text{the number of infected in a sample of 200 individuals}
$$

Assume that the probability of infection is known to be $p = 0.01$.

Then, $$X \sim \text{Binom}(200,0.01).$$

::: {style="color: red;"}
$\star$ This is a Binomial r.v. because it is counting the number of infected ("success") in a fixed number of independent Bernoulli trials.
:::
